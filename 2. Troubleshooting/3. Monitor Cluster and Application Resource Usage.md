## Topic: Monitor Cluster and Application Resource Usage

### 1. Theory
Monitoring cluster and application resource usage is essential for maintaining Kubernetes cluster health and ensuring applications run efficiently. This topic focuses on tracking CPU, memory, and other resource metrics to diagnose performance issues, scheduling failures, and resource contention.

- **Cluster Monitoring**:
  - Involves tracking node-level resources (CPU, memory, disk) to ensure nodes can schedule pods.
  - Relies on the Metrics API, powered by `metrics-server`, for real-time data.
  - Node conditions (e.g., `MemoryPressure`) signal resource constraints affecting scheduling.

- **Application Monitoring**:
  - Focuses on pod-level resource usage, defined by requests and limits.
  - Helps identify overutilized pods, OOM kills, or scheduling issues.
  - Resource quotas and limit ranges enforce namespace-level constraints.

- **Resource Issues**:
  - Common problems include pods stuck in `Pending` due to insufficient resources, OOM kills from exceeding limits, or node contention degrading performance.
  - Understanding eviction policies helps predict pod behavior during scarcity.

**Why It Matters for CKA**:
- The CKA exam tests your ability to monitor and troubleshoot resource-related issues, such as scheduling failures or node overloads, often requiring `kubectl top` and resource adjustments.
- Tasks simulate real-world scenarios, like balancing cluster load or enforcing quotas, demanding fluency with metrics and YAML.

**Big Picture**:
- Monitoring provides visibility into resource bottlenecks, guiding troubleshooting and optimization.
- Cluster-wide metrics (nodes) and pod-level metrics work together to ensure efficient resource allocation.
- Tools like `metrics-server` and `kubectl top` are critical for quick diagnostics, while runtime debugging (e.g., `crictl`) helps resolve low-level issues.

---

### 2. Key Concepts and Components
#### Cluster Monitoring
- **Metrics API**:
  - Exposed by `metrics-server`, collects CPU/memory metrics from kubelet’s cAdvisor.
  - Enables `kubectl top node` and `kubectl top pod`.
  - Deployed in `kube-system` namespace.
- **Commands**:
  - `kubectl top node`: Node CPU/memory usage.
  - `kubectl top pod`: Pod CPU/memory usage (namespace-specific with `-n`).
  - `kubectl describe node`: Node conditions (`MemoryPressure`, `DiskPressure`) and allocable resources.
  - `kubectl get --all-namespaces`: Broad view of resource distribution.
- **Node Conditions**:
  - `MemoryPressure`: Low memory, blocks new pods.
  - `DiskPressure`: Low disk, may evict pods.
  - `PIDPressure`: Too many processes.
  - `OutOfDisk`: No disk space for pods.
- **Impact**:
  - Conditions trigger kubelet actions (e.g., eviction, scheduling blocks).
  - Monitor to prevent node overload and ensure pod placement.

#### Application Monitoring
- **Resource Requests and Limits**:
  - **Requests**: Minimum CPU/memory needed, used for scheduling.
  - **Limits**: Maximum CPU/memory allowed, enforced by container runtime.
  - Example: `requests.cpu: "500m"`, `limits.memory: "1Gi"`.
- **Diagnostics**:
  - `kubectl describe pod`: Shows requests/limits and events (e.g., `FailedScheduling`).
  - `kubectl top pod`: Real-time usage, compares to limits.
- **Resource Quotas**:
  - Namespace-wide limits (e.g., total CPU, memory, pod count).
  - Example: `requests.cpu: "4"`, `limits.memory: "8Gi"`.
- **Limit Ranges**:
  - Per-pod/container defaults or constraints (e.g., max `limits.cpu: "2"`).
  - Prevents resource hogging in namespaces.
- **Monitoring Tools**:
  - Prometheus/Grafana: Advanced metrics (not CKA scope but contextual).
  - CKA focus: `metrics-server` and `kubectl top`.

#### Resource Issues
- **Pods in `Pending`**:
  - Cause: Insufficient node resources (CPU, memory) or taints.
  - Check: `kubectl describe pod`, `kubectl top node`.
- **OOM Kills**:
  - Cause: Container exceeds `limits.memory`, killed by runtime.
  - Check: `kubectl describe pod` (events: `OOMKilled`), `crictl logs`.
- **Resource Contention**:
  - Cause: Overloaded nodes/pods competing for CPU/memory.
  - Check: `kubectl top`, `kubectl describe node`.
- **Eviction Policies**:
  - Kubelet evicts pods based on QoS:
    - **BestEffort**: No requests/limits (first to evict).
    - **Burstable**: Requests < limits.
    - **Guaranteed**: Requests = limits (last to evict).
  - Triggered by node conditions (e.g., `MemoryPressure`).

#### Runtime Debugging
- **Container Runtime**:
  - Manages containers (e.g., containerd, Docker).
  - Issues: OOM kills, resource enforcement failures, runtime crashes.
- **Tools**:
  - `crictl ps`: List containers, check states.
  - `crictl logs <container-id>`: Container logs for errors (e.g., OOM).
  - `crictl inspect <container-id>`: Resource usage, limits.
  - `journalctl -u containerd`: Runtime service logs.
- **Use Cases**:
  - Verify why a container was killed (e.g., memory violation).
  - Debug runtime misconfigurations (e.g., socket errors).
  - Check container state for stuck pods.

#### Exam Relevance
- **High Weight**: Resource monitoring is critical for troubleshooting scheduling and performance issues.
- **Practical Focus**: Expect to deploy `metrics-server`, adjust limits, debug `Pending` pods, and use runtime tools.
- **Version Notes**: v1.29+ uses containerd; `metrics-server` is standard for metrics.

---

### 3. YAML Examples
Below are YAMLs for enabling monitoring, setting quotas, and simulating resource issues.

#### Example 1: Deploy Metrics Server
```yaml
# File: troubleshoot/metrics-server.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: metrics-server
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: system:metrics-server
rules:
- apiGroups: [""]
  resources: ["nodes/metrics", "pods"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:metrics-server
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:metrics-server
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: metrics-server
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: metrics-server
  template:
    metadata:
      labels:
        k8s-app: metrics-server
    spec:
      serviceAccountName: metrics-server
      containers:
      - name: metrics-server
        image: k8s.gcr.io/metrics-server/metrics-server:v0.6.3
        args:
        - --cert-dir=/tmp
        - --secure-port=4443
        - --kubelet-preferred-address-types=InternalIP
        - --kubelet-insecure-tls # For CKA labs
```

**Critical Fields**:
- `image`: Official `metrics-server` image.
- `args`: `--kubelet-insecure-tls` for lab environments (avoids cert issues).
- RBAC: Grants access to node/pod metrics.

#### Example 2: Resource Quota
```yaml
# File: troubleshoot/quota.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: app
spec:
  hard:
    requests.cpu: "2"
    requests.memory: "4Gi"
    limits.cpu: "4"
    limits.memory: "8Gi"
    pods: "10"
```

**Critical Fields**:
- `spec.hard`: Enforces namespace limits.
- `requests.cpu/memory`: Total requested resources.
- `limits.cpu/memory`: Total allowed limits.

#### Example 3: Pod with High Resource Usage
```yaml
# File: troubleshoot/stress-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: stress-pod
  namespace: default
spec:
  containers:
  - name: stress
    image: polinux/stress
    command: ["stress", "--vm", "1", "--vm-bytes", "1500M"]
    resources:
      requests:
        memory: "1Gi"
        cpu: "500m"
      limits:
        memory: "2Gi"
        cpu: "1"
```

**Purpose**: Simulates resource contention for monitoring/debugging.

---

### 4. Critical Commands
Key commands for monitoring and runtime debugging:

| Command | Description | Exam Tip |
|---------|-------------|----------|
| `kubectl top node` | Node CPU/memory usage. | Requires `metrics-server`. |
| `kubectl top pod -n <namespace>` | Pod CPU/memory usage. | Compare to limits. |
| `kubectl describe node <node>` | Node conditions/resources. | Check `MemoryPressure`. |
| `kubectl describe pod <pod>` | Pod events/limits. | Look for `FailedScheduling`. |
| `kubectl get resourcequota -n <namespace>` | Quota details. | Verify enforcement. |
| `crictl ps` | List containers. | Check states (e.g., `OOMKilled`). |
| `crictl logs <container-id>` | Container logs. | Debug OOM or errors. |
| `crictl inspect <container-id>` | Container details. | Verify resource limits. |
| `journalctl -u containerd` | Runtime logs. | Check for runtime issues. |
| `kubectl apply -f metrics-server.yaml` | Deploy `metrics-server`. | Enable `kubectl top`. |

---

### 5. Step-by-Step Diagnostics
Here’s how to monitor and debug resource issues, with extra runtime debugging.

#### Scenario 1: Pod Stuck in `Pending`
**Step 1: Check Pod**
```bash
kubectl get pods -n app
# Output: NAME   READY   STATUS    RESTARTS   AGE
#         app    0/1     Pending   0          1m
```

**Step 2: Inspect Events**
```bash
kubectl describe pod app -n app
# Events: "FailedScheduling: insufficient cpu"
```

**Step 3: Check Node Resources**
```bash
kubectl top node
# Output: NAME     CPU(cores)   MEMORY(bytes)
#         node-1   1800m        3500Mi
#         node-2   200m         500Mi
```

**Step 4: Check Pod Requests**
```bash
kubectl describe pod app -n app
# Resources: requests.cpu: 2
```

**Step 5: Runtime Debugging**
```bash
kubectl apply -f troubleshoot/debug-pod.yaml
kubectl exec -it debug-pod -n kube-system -- sh
crictl ps
# Confirm: No container for app (not scheduled)
journalctl -u containerd
# Look for: No runtime errors (issue is scheduling, not runtime)
```

**Step 6: Fix**
```bash
kubectl edit pod app -n app
# Reduce: resources.requests.cpu: 500m
kubectl get pods -n app
# Output: STATUS: Running
```

#### Scenario 2: OOM Kill
**Step 1: Check Pod**
```bash
kubectl get pods
# Output: NAME         READY   STATUS             RESTARTS   AGE
#         stress-pod   0/1     CrashLoopBackOff   3          2m
```

**Step 2: Inspect Events**
```bash
kubectl describe pod stress-pod
# Events: "OOMKilled"
```

**Step 3: Runtime Debugging**
```bash
kubectl exec -it debug-pod -n kube-system -- sh
crictl ps -a
# Output: Container ID   STATE    NAME
#         abc123        Exited   stress
crictl logs abc123
# Output: "memory allocation failed"
crictl inspect abc123
# Output: memory.limit_in_bytes: 2Gi
```

**Step 4: Check Usage**
```bash
kubectl top pod stress-pod
# Output: CPU: 100m   MEMORY: 1800Mi (near limit)
```

**Step 5: Fix**
```bash
kubectl edit pod stress-pod
# Increase: resources.limits.memory: 3Gi
kubectl get pods
# Output: STATUS: Running
```

---

### 6. Exam-Style Questions
Below are CKA-style questions, with runtime debugging included.

#### Question 1: Task-Based (6 minutes)
**Task**: Deploy `metrics-server` to enable `kubectl top`.

**Steps**:
```bash
kubectl apply -f troubleshoot/metrics-server.yaml
kubectl get pods -n kube-system | grep metrics-server
# Output: STATUS: Running
kubectl top node
# Output: Shows CPU/memory
```

#### Question 2: Troubleshooting (8 minutes)
**Task**: Debug a pod stuck in `Pending` due to insufficient resources.

**Steps**:
```bash
kubectl get pods -n app
# Output: NAME   READY   STATUS    RESTARTS   AGE
#         app    0/1     Pending   0          1m

kubectl describe pod app -n app
# Events: "FailedScheduling: insufficient memory"

kubectl top node
# Output: node-1   1900Mi used   100Mi free

# Runtime check
kubectl exec -it debug-pod -n kube-system -- sh
crictl ps
# No container (not scheduled)

# Fix
kubectl edit pod app -n app
# Reduce: resources.requests.memory: 50Mi

# Verify
kubectl get pods -n app
# Output: STATUS: Running
```

#### Question 3: Validation (5 minutes)
**Task**: Confirm a resource quota is enforced in namespace `app`.

**Steps**:
```bash
kubectl apply -f troubleshoot/quota.yaml
kubectl create -f troubleshoot/stress-pod.yaml -n app
kubectl get pods -n app
# Output: STATUS: Pending (quota exceeded)

kubectl describe resourcequota compute-quota -n app
# Output: limits.memory: 8Gi/8Gi used
```

#### Question 4: Runtime Debugging (7 minutes)
**Task**: Debug a pod killed by OOM, using runtime tools.

**Steps**:
```bash
kubectl get pods
# Output: stress-pod   0/1   CrashLoopBackOff

kubectl describe pod stress-pod
# Events: "OOMKilled"

# Runtime debug
kubectl exec -it debug-pod -n kube-system -- sh
crictl ps -a
# Output: Container ID   STATE    NAME
#         xyz789        Exited   stress
crictl inspect xyz789
# Output: memory.limit_in_bytes: 2Gi
crictl logs xyz789
# Output: "out of memory"

# Fix
kubectl edit pod stress-pod
# Increase: limits.memory: 3Gi

# Verify
kubectl get pods
# Output: STATUS: Running
```

---

### 7. Important Key Points to Remember
- **Cluster Monitoring**:
  - `metrics-server`: Enables `kubectl top`.
  - `kubectl top node/pod`: Real-time CPU/memory.
  - Conditions: `MemoryPressure`, `DiskPressure` block scheduling.
- **Application Monitoring**:
  - Requests: Scheduling needs.
  - Limits: Runtime caps, prevent OOM.
  - Quotas/LimitRanges: Namespace constraints.
- **Resource Issues**:
  - `Pending`: Insufficient resources, taints.
  - OOM: Exceeds `limits.memory`.
  - Eviction: BestEffort > Burstable > Guaranteed.
- **Runtime Debugging**:
  - `crictl ps/logs/inspect`: Container states, errors, limits.
  - `journalctl -u containerd`: Runtime service issues.
  - Use for OOM, stuck containers, runtime crashes.
- **Exam Focus**:
  - Deploy `metrics-server`, set quotas.
  - Debug `Pending`, OOM with `kubectl`, `crictl`.
  - Validate with `kubectl top`.
- **Version Note**:
  - v1.29+: Containerd, `metrics-server` v0.6+.

---

### 8. Common Mistakes to Avoid
- **Mistake**: Forgetting to deploy `metrics-server` for `kubectl top`.
  - **Fix**: Apply `metrics-server.yaml`.
- **Mistake**: Setting requests > limits in pod spec.
  - **Fix**: Ensure `requests` ≤ `limits`.
- **Mistake**: Ignoring runtime logs for OOM kills.
  - **Fix**: Use `crictl logs/inspect`.
- **Mistake**: Applying quota in wrong namespace.
  - **Fix**: Use `-n` or `metadata.namespace`.
- **Mistake**: Not checking node conditions for `Pending` pods.
  - **Fix**: Run `kubectl describe node`.

**Exam Traps**:
- Missing `--all-namespaces` in `kubectl top pod`.
- Not validating quota enforcement.
- Overlooking runtime errors vs. scheduling issues.

---

### 9. Troubleshooting Tips
- **Pod Pending**:
  - Check: `kubectl describe pod`, `kubectl top node`.
  - Causes: Insufficient CPU/memory, taints.
  - Fix: Reduce requests, remove taints.
- **OOM Kill**:
  - Check: `kubectl describe pod`, `crictl logs`.
  - Causes: Low `limits.memory`, app leaks.
  - Fix: Increase limits, optimize app.
- **Node Overload**:
  - Check: `kubectl top node`, `kubectl describe node`.
  - Causes: Too many pods, high requests.
  - Fix: Reschedule pods, scale nodes.
- **Runtime Issues**:
  - Check: `crictl ps/logs/inspect`, `journalctl -u containerd`.
  - Causes: OOM, socket errors, image issues.
  - Fix: Adjust limits, restart containerd.
- **Tools**:
  - `kubectl top/describe`: Metrics, events.
  - `crictl`: Container states, logs.
  - `journalctl`: Runtime/service logs.

**Debugging Checklist**:
1. Check pod status (`kubectl get pods`).
2. Review events (`kubectl describe pod`).
3. Monitor usage (`kubectl top`).
4. Debug runtime (`crictl ps/logs`).
5. Validate fixes (`kubectl get`).

---

### 10. Additional Resources
- **Kubernetes Docs**:
  - [Resource Monitoring](https://kubernetes.io/docs/tasks/debug/debug-cluster/resource-usage-monitoring/)
  - [Metrics Server](https://github.com/kubernetes-sigs/metrics-server)
  - [Resource Quotas](https://kubernetes.io/docs/concepts/policy/resource-quotas/)
- **Practice Tools**:
  - **Minikube**: Test `metrics-server`, quotas.
  - **KillerCoda/KodeKloud**: CKA labs.
  - **Kind**: Multi-node resource scenarios.
- **Community**:
  - CNCF Slack: #kubernetes-users, #cka-prep.
  - Kubernetes GitHub: Metrics SIG.
  - X posts: Search #KubernetesMonitoring, #CKA.

---

### 11. GitHub Repo Integration
Here’s how to organize this topic in your GitHub repo.

#### Folder Structure
```
cka-prep/
├── storage/
├── troubleshoot/
│   ├── debug-pod.yaml
│   ├── metrics-server.yaml
│   ├── quota.yaml
│   ├── stress-pod.yaml
│   ├── README.md
├── README.md
```

#### README Content (troubleshoot/README.md, Updated)
```markdown
# Troubleshoot: Monitor Cluster and Application Resource Usage

## Theory
- **Cluster**: Monitor nodes with `kubectl top node`, conditions.
- **Application**: Pod requests/limits, quotas.
- **Issues**: `Pending` pods, OOM kills, contention.

## YAML Examples
- `metrics-server.yaml`: Enable `kubectl top`.
- `quota.yaml`: Namespace resource limits.
- `stress-pod.yaml`: Simulate resource issues.
- `debug-pod.yaml`: Runtime debugging.

## Diagnostics
1. Check pods: `kubectl get pods`
2. Monitor: `kubectl top node/pod`
3. Events: `kubectl describe pod/node`
4. Runtime: `crictl ps/logs`
5. Fix: Adjust limits, quotas.

## Key Points
- `metrics-server` for `kubectl top`.
- Quotas limit namespace resources.
- OOM kills from low `limits.memory`.
- `crictl` for runtime debugging.

## Common Mistakes
- No `metrics-server`.
- Wrong namespace for quotas.
- Ignoring `crictl` for OOM.

## Troubleshooting
- `Pending`? Check `kubectl top`, `describe`.
- OOM? Use `crictl logs`, increase limits.

## Questions
1. Deploy `metrics-server`.
2. Debug `Pending` pod.
3. Validate quota.
4. Debug OOM with `crictl`.
```

#### File Comments (stress-pod.yaml)
```yaml
# stress-pod.yaml
# Simulates high resource usage for monitoring
# Verify: kubectl top pod stress-pod
# Use: Debug OOM, `Pending`, runtime issues
```

---

### Comprehensive Summary
This topic equips you to monitor and troubleshoot resource usage, critical for cluster stability. You’ve learned:
- How to deploy `metrics-server` and use `kubectl top` for monitoring.
- How to manage pod requests/limits and namespace quotas.
- How to debug `Pending` pods, OOM kills, and contention, with runtime tools like `crictl`.
- Extra runtime debugging for container states, logs, and limits.

**Practice Plan**:
- Deploy `metrics-server.yaml` and test `kubectl top` (Minikube, Kind).
- Apply `quota.yaml` and `stress-pod.yaml` to simulate issues.
- Debug with `crictl ps/logs/inspect` for OOM and runtime failures.
- Time yourself on the exam questions (<20 minutes total).

**Next Steps**:
- Move to the next troubleshooting topic (e.g., “Troubleshoot Application Failures”).
- Practice mixed scenarios (e.g., quotas + runtime issues).
- Let me know if you want more runtime debugging or other focus areas.

---

This response covers **Monitor Cluster and Application Resource Usage** comprehensively, with extra runtime debugging as requested, tailored for your CKA prep and GitHub repo. We’re making awesome progress! Please share the next topic, and I’ll keep the deep dives coming. Any tweaks or more runtime scenarios? Let’s keep this prep on fire! 😊
