## Topic: Configure Pod Admission and Scheduling (Limits, Node Affinity, etc.)

### 1. Theory
Kubernetes provides mechanisms to control **pod admission and scheduling**, ensuring pods are placed on appropriate nodes based on resource needs, node characteristics, and cluster policies. This topic covers **resource requests/limits**, **node affinity/anti-affinity**, **taints/tolerations**, and **node selectors** to achieve precise scheduling.

- **Resource Limits and Requests**:
  - Define pod resource requirements (`requests`) and caps (`limits`) for CPU and memory.
  - Influence scheduling and prevent resource contention.

- **Node Affinity and Anti-Affinity**:
  - Schedule pods on nodes matching specific labels or co-locate/separate pods.
  - Offer fine-grained control over placement.

- **Taints and Tolerations**:
  - Restrict pod scheduling to specific nodes unless pods tolerate taints.
  - Enable node specialization (e.g., dedicated nodes).

- **Node Selectors**:
  - Simple label-based scheduling for basic node targeting.

**Why It Matters for CKA**:
- Scheduling is a core CKA topic, testing your ability to configure pod placement, manage resources, and troubleshoot scheduling failures under time pressure.
- Tasks often involve setting limits, applying affinity rules, managing taints, or fixing unschedulable pods, reflecting real-world cluster optimization needs.

**Big Picture**:
- Resource requests/limits ensure efficient node allocation and pod stability.
- Affinity and taints provide advanced scheduling control for complex environments.
- Troubleshooting scheduling issues often involves checking node labels, taints, or resource availability, sometimes requiring runtime debugging.

---

### 2. Key Concepts and Components
#### Resource Limits and Requests
- **Purpose**: Define pod resource needs and constraints.
- **Key Fields** (`spec.containers.resources`):
  - **requests**:
    - Minimum CPU/memory needed (e.g., `cpu: "100m"`, `memory: "128Mi"`).
    - Used by scheduler to place pods on nodes with sufficient capacity.
  - **limits**:
    - Maximum CPU/memory allowed (e.g., `cpu: "500m"`, `memory: "512Mi"`).
    - Prevents overuse (CPU throttling, OOM kills for memory).
- **Units**:
  - CPU: `m` (millicores, `100m` = 0.1 core), cores (e.g., `1`).
  - Memory: `Mi` (mebibytes), `Gi` (gibibytes).
- **Behavior**:
  - Pods without `requests` may oversubscribe nodes.
  - Exceeding `limits.memory` triggers OOM kill; `limits.cpu` throttles.
- **Example**: Request `100m` CPU, limit to `200m` to avoid overuse.

#### Node Affinity and Anti-Affinity
- **Purpose**: Schedule pods on nodes or with/away from other pods based on labels.
- **Node Affinity** (`spec.affinity.nodeAffinity`):
  - **requiredDuringSchedulingIgnoredDuringExecution**: Hard rule, pod must match.
  - **preferredDuringSchedulingIgnoredDuringExecution**: Soft rule, prefer matching nodes.
  - Matches node labels (e.g., `disk=ssd`).
- **Pod Affinity/Anti-Affinity** (`spec.affinity.podAffinity/podAntiAffinity`):
  - Co-locate pods (affinity, e.g., app + cache) or separate (anti-affinity, e.g., replicas).
  - Uses `topologyKey` (e.g., `kubernetes.io/hostname` for same node).
- **Key Fields**:
  - `labelSelector`: Match pod/node labels.
  - `topologyKey`: Scope (e.g., node, rack, zone).
- **Example**: Schedule app on `env=prod` nodes, avoid co-locating replicas.

#### Taints and Tolerations
- **Taints**:
  - **Purpose**: Repel pods from nodes unless tolerated.
  - **Applied**: Via `kubectl taint` (e.g., `key=value:NoSchedule`).
  - **Effects**:
    - `NoSchedule`: Prevent scheduling.
    - `PreferNoSchedule`: Avoid if possible.
    - `NoExecute`: Evict non-tolerating pods.
- **Tolerations** (`spec.tolerations`):
  - **Purpose**: Allow pods to schedule on tainted nodes.
  - **Fields**:
    - `key`, `value`: Match taint.
    - `operator`: `Equal`, `Exists`.
    - `effect`: `NoSchedule`, `NoExecute`.
    - `tolerationSeconds`: Time before eviction (`NoExecute`).
- **Example**: Taint node for GPU pods, tolerate for specific apps.

#### Node Selectors
- **Purpose**: Simple scheduling on nodes with specific labels.
- **Field**: `spec.nodeSelector` (e.g., `env: prod`).
- **Comparison**:
  - Less flexible than node affinity (no soft rules, complex expressions).
  - Easier for basic needs.
- **Example**: Schedule pod on nodes labeled `disk=ssd`.

#### Runtime Debugging
- **Use Case**: Troubleshoot unschedulable pods (e.g., `Pending` due to taints).
- **Tools**:
  - `crictl ps`: Check pod states (e.g., `Pending`).
  - `crictl inspect <container-id>`: Verify scheduling configs.
  - `journalctl -u kubelet`: Scheduler errors.
- **Relevance**: Diagnose taint, affinity, or resource issues.

#### Exam Relevance
- **High Weight**: Scheduling is critical, testing resource configs and placement rules.
- **Practical Focus**: Expect to set limits, apply taints, configure affinity, and fix scheduling errors.
- **Version Notes**: v1.29+ uses containerd, with unchanged scheduling mechanics but `crictl` for debugging.

---

### 3. YAML Examples
Below are YAMLs for pods and Deployments with scheduling configs.

#### Example 1: Pod with Resource Limits
```yaml
# File: workloads/limited-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: limited-app
  namespace: default
spec:
  containers:
  - name: app
    image: nginx:1.25
    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"
      limits:
        cpu: "200m"
        memory: "256Mi"
```

**Critical Fields**:
- `resources.requests`: Minimum needs.
- `resources.limits`: Maximum allowed.

#### Example 2: Deployment with Node Affinity
```yaml
# File: workloads/affinity-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: affinity-app
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: affinity
  template:
    metadata:
      labels:
        app: affinity
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: env
                operator: In
                values:
                - prod
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: affinity
              topologyKey: kubernetes.io/hostname
      containers:
      - name: app
        image: nginx:1.25
```

**Critical Fields**:
- `nodeAffinity.required`: Must schedule on `env=prod`.
- `podAntiAffinity.preferred`: Avoid same node for replicas.

#### Example 3: Pod with Toleration
```yaml
# File: workloads/toleration-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: toleration-app
  namespace: default
spec:
  tolerations:
  - key: "app"
    operator: "Equal"
    value: "gpu"
    effect: "NoSchedule"
  containers:
  - name: app
    image: nginx:1.25
```

**Critical Fields**:
- `tolerations`: Allows scheduling on `app=gpu:NoSchedule`.

#### Example 4: Pod with Node Selector
```yaml
# File: workloads/selector-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: selector-app
  namespace: default
spec:
  nodeSelector:
    disk: ssd
  containers:
  - name: app
    image: nginx:1.25
```

**Critical Fields**:
- `nodeSelector`: Targets `disk=ssd`.

#### Example 5: Debug Pod
```yaml
# File: workloads/debug-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: debug-pod
  namespace: default
spec:
  containers:
  - name: debug
    image: busybox
    command: ["sh", "-c", "sleep 3600"]
```

**Purpose**: Debug scheduling issues.

---

### 4. Critical Commands
Key commands for pod admission and scheduling:

| Command | Description | Exam Tip |
|---------|-------------|----------|
| `kubectl apply -f <file>` | Apply pod/Deployment. | Set up scheduling. |
| `kubectl get pods -o wide` | Pod node placement. | Verify scheduling. |
| `kubectl describe pod <pod>` | Scheduling events. | Debug issues. |
| `kubectl taint nodes <node> <key>=<value>:<effect>` | Taint node. | Restrict pods. |
| `kubectl taint nodes <node> <key>-` | Remove taint. | Allow pods. |
| `kubectl label nodes <node> <key>=<value>` | Label node. | Enable selectors. |
| `kubectl edit pod <pod>` | Fix scheduling. | Adjust rules. |
| `kubectl describe node <node>` | Taints, labels. | Check node state. |
| `kubectl top pods` | Resource usage. | Verify limits. |
| `crictl ps` | Pod states. | Debug Pending. |
| `crictl inspect <id>` | Scheduling configs. | Verify rules. |

---

### 5. Step-by-Step Procedures
Hereâ€™s how to configure, test, and troubleshoot pod scheduling.

#### Scenario 1: Configure Resource Limits
**Step 1: Deploy Pod**
```bash
kubectl apply -f workloads/limited-pod.yaml
kubectl get pods
# Output: limited-app   1/1   Running
```

**Step 2: Verify**
```bash
kubectl describe pod limited-app
# Output: Requests: cpu: 100m, memory: 128Mi
#         Limits: cpu: 200m, memory: 256Mi
kubectl top pod limited-app
# Output: CPU: <200m, Memory: <256Mi
```

#### Scenario 2: Apply Taint and Toleration
**Step 1: Taint Node**
```bash
kubectl taint nodes node1 app=gpu:NoSchedule
kubectl describe node node1
# Output: Taints: app=gpu:NoSchedule
```

**Step 2: Deploy Pod without Toleration**
```bash
cat <<EOF > no-toleration-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: no-toleration-app
spec:
  containers:
  - name: app
    image: nginx:1.25
EOF
kubectl apply -f no-toleration-pod.yaml
kubectl get pods
# Output: no-toleration-app   0/1   Pending
```

**Step 3: Deploy Pod with Toleration**
```bash
kubectl apply -f workloads/toleration-pod.yaml
kubectl get pods -o wide
# Output: toleration-app   1/1   Running   node1
```

**Step 4: Debug Pending Pod**
```bash
kubectl describe pod no-toleration-app
# Output: Events: "No nodes match pod tolerations"
kubectl apply -f workloads/debug-pod.yaml
kubectl exec -it debug-pod -- sh
crictl ps -a
# Output: no-toleration-app   Pending
```

**Step 5: Fix**
```bash
kubectl taint nodes node1 app=gpu:NoSchedule-
kubectl get pods
# Output: no-toleration-app   1/1   Running
```

#### Scenario 3: Configure Node Affinity
**Step 1: Label Node**
```bash
kubectl label nodes node1 env=prod
kubectl describe node node1
# Output: Labels: env=prod
```

**Step 2: Deploy**
```bash
kubectl apply -f workloads/affinity-deployment.yaml
kubectl get pods -o wide
# Output: affinity-app-xyz   1/1   Running   node1
```

**Step 3: Verify Anti-Affinity**
```bash
kubectl get pods -o wide
# Output: Pods on different nodes (if possible)
```

#### Scenario 4: Debug Unschedulable Pod
**Step 1: Deploy Faulty Pod**
```bash
cat <<EOF > bad-affinity-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: bad-affinity-app
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: env
            operator: In
            values:
            - non-existent
  containers:
  - name: app
    image: nginx:1.25
EOF
kubectl apply -f bad-affinity-pod.yaml
```

**Step 2: Check**
```bash
kubectl get pods
# Output: bad-affinity-app   0/1   Pending
kubectl describe pod bad-affinity-app
# Output: Events: "No nodes match node affinity"
```

**Step 3: Fix**
```bash
kubectl edit pod/bad-affinity-app
# Correct: values: ["prod"]
kubectl get pods
# Output: bad-affinity-app   1/1   Running
```

---

### 6. Exam-Style Questions
Below are CKA-style questions, with runtime debugging included.

#### Question 1: Task-Based (6 minutes)
**Task**: Configure a pod with CPU/memory limits.

**Steps**:
```bash
kubectl apply -f workloads/limited-pod.yaml
kubectl describe pod limited-app
# Output: Limits: cpu: 200m, memory: 256Mi
kubectl get pods
# Output: limited-app   1/1   Running
```

#### Question 2: Task-Based (7 minutes)
**Task**: Taint a node and add toleration.

**Steps**:
```bash
kubectl taint nodes node1 app=gpu:NoSchedule
kubectl apply -f workloads/toleration-pod.yaml
kubectl get pods -o wide
# Output: toleration-app   1/1   Running   node1
```

#### Question 3: Troubleshooting (8 minutes)
**Task**: Fix a pod stuck in `Pending` due to taint.

**Steps**:
```bash
kubectl apply -f no-toleration-pod.yaml
kubectl get pods
# Output: no-toleration-app   0/1   Pending

kubectl describe pod no-toleration-app
# Output: "No nodes match tolerations"

# Fix
kubectl apply -f workloads/toleration-pod.yaml
kubectl delete pod no-toleration-app
kubectl apply -f workloads/toleration-pod.yaml
kubectl get pods
# Output: toleration-app   1/1   Running
```

#### Question 4: Validation (5 minutes)
**Task**: Verify affinity schedules pods correctly.

**Steps**:
```bash
kubectl label nodes node1 env=prod
kubectl apply -f workloads/affinity-deployment.yaml
kubectl get pods -o wide
# Output: affinity-app-xyz   1/1   Running   node1
```

---

### 7. Important Key Points to Remember
- **Resources**:
  - `requests`: Minimum, used for scheduling.
  - `limits`: Maximum, prevents overuse.
- **Node Affinity**:
  - `required`: Hard rule.
  - `preferred`: Soft rule.
  - Pod affinity: Co-locate/separate.
- **Taints/Tolerations**:
  - `NoSchedule`, `NoExecute`, `PreferNoSchedule`.
  - Tolerations allow scheduling.
- **Node Selector**:
  - Simple label matching.
  - Less flexible than affinity.
- **Runtime Debugging**:
  - `crictl ps/inspect`: Scheduling issues.
  - Use for Pending pods.
- **Exam Focus**:
  - Configure limits, affinity, taints.
  - Fix unschedulable pods.
  - Validate placement.

---

### 8. Common Mistakes to Avoid
- **Mistake**: Missing `requests` causing overscheduling.
  - **Fix**: Add `resources.requests`.
- **Mistake**: Wrong taint effect (e.g., `NoExecute` evicts).
  - **Fix**: Use `NoSchedule` unless eviction needed.
- **Mistake**: Incorrect affinity labels.
  - **Fix**: Verify node labels.
- **Mistake**: No toleration for tainted node.
  - **Fix**: Add `tolerations`.
- **Mistake**: Ignoring runtime errors.
  - **Fix**: Check `crictl inspect`.

**Exam Traps**:
- Wrong label keys/values.
- Missing `topologyKey` in affinity.
- Overlooking node capacity.

---

### 9. Troubleshooting Tips
- **Pod Pending**:
  - Check: `kubectl describe pod`.
  - Causes: Taints, affinity, insufficient resources.
  - Fix: Add tolerations, adjust affinity, check nodes.
- **Wrong Node Placement**:
  - Check: `kubectl get pods -o wide`.
  - Causes: Wrong `nodeSelector`, affinity.
  - Fix: Correct labels, rules.
- **OOM Kills**:
  - Check: `kubectl describe pod`, `crictl logs`.
  - Causes: Low `limits.memory`.
  - Fix: Increase limits.
- **Runtime Issues**:
  - Check: `crictl ps/inspect`.
  - Causes: Scheduler errors, node issues.
  - Fix: Adjust configs, restart runtime.
- **Tools**:
  - `kubectl describe`: Scheduling events.
  - `kubectl top`: Resource usage.
  - `crictl`: Pod diagnostics.

**Debugging Checklist**:
1. Check pod (`kubectl get pods`).
2. Inspect events (`kubectl describe pod`).
3. Verify nodes (`kubectl describe node`).
4. Debug runtime (`crictl inspect`).
5. Fix YAML (`kubectl edit`).
6. Validate (`kubectl get pods -o wide`).

---

### 10. Additional Resources
- **Kubernetes Docs**:
  - [Resource Management](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)
  - [Node Affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity)
  - [Taints and Tolerations](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/)
  - [Node Selector](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
- **Practice Tools**:
  - **Minikube**: Test scheduling.
  - **KillerCoda/KodeKloud**: CKA labs.
  - **Kind**: Multi-node setups.
- **Community**:
  - CNCF Slack: #kubernetes-users, #cka-prep.
  - Kubernetes GitHub: Scheduling issues.
  - X posts: Search #KubernetesScheduling, #CKA.

---

### 11. GitHub Repo Integration
Hereâ€™s how to organize this topic in your GitHub repo.

#### Folder Structure
```
cka-prep/
â”œâ”€â”€ storage/
â”œâ”€â”€ troubleshoot/
â”œâ”€â”€ workloads/
â”‚   â”œâ”€â”€ my-app-deployment.yaml
â”‚   â”œâ”€â”€ rolling-deployment.yaml
â”‚   â”œâ”€â”€ debug-pod.yaml
â”‚   â”œâ”€â”€ private-deployment.yaml
â”‚   â”œâ”€â”€ registry-secret.yaml
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ logging-daemonset.yaml
â”‚   â”œâ”€â”€ backup-cronjob.yaml
â”‚   â”œâ”€â”€ batch-job.yaml
â”‚   â”œâ”€â”€ sidecar-pod.yaml
â”‚   â”œâ”€â”€ init-pod.yaml
â”‚   â”œâ”€â”€ blue-deployment.yaml
â”‚   â”œâ”€â”€ green-deployment.yaml
â”‚   â”œâ”€â”€ blue-green-service.yaml
â”‚   â”œâ”€â”€ stable-deployment.yaml
â”‚   â”œâ”€â”€ canary-deployment.yaml
â”‚   â”œâ”€â”€ canary-service.yaml
â”‚   â”œâ”€â”€ rolling-update-deployment.yaml
â”‚   â”œâ”€â”€ recreate-deployment.yaml
â”‚   â”œâ”€â”€ app-configmap.yaml
â”‚   â”œâ”€â”€ app-secret.yaml
â”‚   â”œâ”€â”€ configured-pod.yaml
â”‚   â”œâ”€â”€ private-pod.yaml
â”‚   â”œâ”€â”€ immutable-configmap.yaml
â”‚   â”œâ”€â”€ config-pod.yaml
â”‚   â”œâ”€â”€ tls-secret.yaml
â”‚   â”œâ”€â”€ secret-pod.yaml
â”‚   â”œâ”€â”€ scalable-deployment.yaml
â”‚   â”œâ”€â”€ web-hpa.yaml
â”‚   â”œâ”€â”€ load-pod.yaml
â”‚   â”œâ”€â”€ robust-deployment.yaml
â”‚   â”œâ”€â”€ slow-pod.yaml
â”‚   â”œâ”€â”€ limited-pod.yaml
â”‚   â”œâ”€â”€ affinity-deployment.yaml
â”‚   â”œâ”€â”€ toleration-pod.yaml
â”‚   â”œâ”€â”€ selector-pod.yaml
â”‚   â”œâ”€â”€ README.md
â”œâ”€â”€ README.md
```

#### README Content (workloads/README.md, Updated)
```markdown
# Workloads & Scheduling: Pod Admission and Scheduling

## Theory
- **Resources**: Requests for scheduling, limits for control.
- **Affinity**: Node/pod placement rules.
- **Taints**: Restrict scheduling.
- **Node Selector**: Simple label matching.

## Files
- `limited-pod.yaml`: Resource limits.
- `affinity-deployment.yaml`: Node/pod affinity.
- `toleration-pod.yaml`: Tolerates taint.
- `selector-pod.yaml`: Node selector.
- Others: `robust-deployment.yaml`, etc.

## Procedures
1. Deploy: `kubectl apply -f`
2. Taint: `kubectl taint`
3. Label: `kubectl label`
4. Debug: `kubectl describe`, `crictl`
5. Validate: `kubectl get pods -o wide`

## Key Points
- `requests` ensure scheduling.
- Affinity: `required` vs. `preferred`.
- Taints: `NoSchedule`, `NoExecute`.

## Common Mistakes
- Missing `requests`.
- Wrong affinity labels.
- No tolerations.

## Troubleshooting
- Pending? Check taints, affinity.
- Wrong node? Fix labels, selectors.

## Questions
1. Set limits.
2. Add toleration.
3. Fix Pending pod.
4. Verify affinity.
```

#### File Comments (affinity-deployment.yaml)
```yaml
# affinity-deployment.yaml
# Deployment with node and pod anti-affinity
# Verify: kubectl get pods -o wide
# Use: Practice scheduling, affinity rules
```

---

### Comprehensive Summary
This topic equips you to configure **pod admission and scheduling**, optimizing resource usage and placement with **requests/limits**, **affinity**, **taints/tolerations**, and **node selectors**. Youâ€™ve learned:
- How to set resource constraints for scheduling and stability.
- How to use affinity and taints for precise pod placement.
- How to troubleshoot scheduling failures using `kubectl` and `crictl`.
- Practical skills for validating node assignments and resource enforcement.

**Practice Plan**:
- Deploy `limited-pod.yaml`, `affinity-deployment.yaml`, `toleration-pod.yaml`, and `selector-pod.yaml` (Minikube, Kind).
- Simulate failures: taint nodes, use wrong labels. Debug with `kubectl describe`, `crictl`.
- Time yourself on the exam questions (<20 minutes total).
- Use KillerCoda/KodeKloud for scheduling labs.

**Next Steps**:
- Move to the next topic in Workloads & Scheduling (if any remain) or the next CKA section (e.g., **Cluster Architecture, Installation & Configuration**).
- Practice mixed scenarios (e.g., affinity + probes).
- Let me know if you want more runtime debugging, scheduling edge cases, or a section review.

---

This response covers **Configure Pod Admission and Scheduling** comprehensively, with runtime debugging included, tailored for your CKA prep and GitHub repo. Weâ€™re dominating Workloads & Schedulingâ€”great job! Please share the next topic or section, and Iâ€™ll keep delivering. Any tweaks, more debugging, or specific scheduling scenarios? Letâ€™s keep this prep phenomenal! ðŸ˜Š
