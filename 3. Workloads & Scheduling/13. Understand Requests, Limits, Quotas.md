## Topic: Understand Requests, Limits, Quotas

### 1. Theory
Kubernetes provides mechanisms to manage resource allocation and constraints for pods and namespaces, ensuring efficient and stable cluster operation. This topic covers **requests** and **limits** for pod-level resource management, **ResourceQuotas** for namespace-level limits, and **LimitRanges** for default constraints.

- **Requests and Limits**:
  - Define minimum resource needs (`requests`) and maximum usage (`limits`) for pods.
  - Influence scheduling and prevent resource contention.

- **Resource Quotas**:
  - Restrict total resource usage (e.g., CPU, memory, pods) in a namespace.
  - Prevent overuse by limiting pod creation and scaling.

- **Limit Ranges**:
  - Set default, minimum, and maximum requests/limits for pods in a namespace.
  - Ensure consistent resource policies without manual configuration.

**Why It Matters for CKA**:
- Resource management is a core CKA topic, testing your ability to configure pod resources, enforce namespace constraints, and troubleshoot failures under time pressure.
- Tasks often involve setting requests/limits, creating quotas, or fixing scheduling issues, reflecting real-world needs for cluster efficiency.

**Big Picture**:
- Requests/limits balance scheduling and pod stability.
- ResourceQuotas and LimitRanges enforce policies at scale, preventing resource exhaustion.
- Troubleshooting often involves checking resource availability, quota violations, or runtime errors.

---

### 2. Key Concepts and Components
#### Requests and Limits
- **Purpose**: Control pod resource allocation.
- **Key Fields** (`spec.containers.resources`):
  - **requests**:
    - Minimum CPU/memory needed (e.g., `cpu: "500m"`, `memory: "512Mi"`).
    - Used by scheduler to place pods on nodes with sufficient capacity.
  - **limits**:
    - Maximum CPU/memory allowed (e.g., `cpu: "1"`, `memory: "1Gi"`).
    - Enforces caps (CPU throttling, memory OOM kills).
- **Units**:
  - **CPU**: `m` (millicores, `500m` = 0.5 cores), cores (e.g., `1`).
  - **Memory**: `Mi` (mebibytes), `Gi` (gibibytes).
- **Behavior**:
  - **Requests**: Sum must fit node’s allocatable resources.
  - **Limits**: Exceeding `limits.memory` kills pod; `limits.cpu` throttles usage.
  - Pods without `requests` may overschedule; without `limits`, may overuse.
- **Example**: Request `500m` CPU, limit to `1` CPU to cap usage.

#### Resource Quotas
- **Purpose**: Limit total resource usage in a namespace.
- **Key Fields** (`spec.hard`):
  - `requests.cpu`, `requests.memory`: Total requests.
  - `limits.cpu`, `limits.memory`: Total limits.
  - `pods`: Maximum pod count.
  - `count/<resource>`: Limit objects (e.g., `count/pods`).
- **Behavior**:
  - Prevents new pods if quota exceeded (e.g., `requests.cpu > 4`).
  - Applies to all pods in namespace, including Deployments.
- **Example**: Limit namespace to 10 pods, 4Gi memory.

#### Limit Ranges
- **Purpose**: Set default/min/max requests and limits per pod/container.
- **Key Fields** (`spec.limits`):
  - **default**: Applied if no `limits` specified (e.g., `cpu: "500m"`).
  - **defaultRequest**: Applied if no `requests` specified.
  - **min**, **max**: Enforce boundaries (e.g., `min.cpu: "100m"`).
- **Behavior**:
  - Overrides pod specs to enforce constraints.
  - Prevents creation if outside min/max.
- **Example**: Default pod to `256Mi` memory, max `1Gi`.

#### Runtime Debugging
- **Use Case**: Troubleshoot pods failing to schedule (e.g., quota violations, OOM).
- **Tools**:
  - `crictl ps`: Check pod states (e.g., `Pending`, `Terminated`).
  - `crictl logs <container-id>`: Fetch OOM or crash errors.
  - `crictl inspect <container-id>`: Verify resource configs.
  - `journalctl -u kubelet`: Scheduler or OOM events.
- **Relevance**: Diagnose resource-related failures.

#### Exam Relevance
- **Moderate Weight**: Resource management is common, testing pod and namespace configs.
- **Practical Focus**: Expect to set requests/limits, create quotas, fix scheduling errors, and validate constraints.
- **Version Notes**: v1.29+ uses containerd, with unchanged resource mechanics but `crictl` for debugging.

---

### 3. YAML Examples
Below are YAMLs for pods, quotas, and limit ranges.

#### Example 1: Pod with Requests and Limits
```yaml
# File: workloads/resource-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: resource-app
  namespace: default
spec:
  containers:
  - name: app
    image: nginx:1.25
    resources:
      requests:
        cpu: "500m"
        memory: "512Mi"
      limits:
        cpu: "1"
        memory: "1Gi"
```

**Critical Fields**:
- `requests`: Minimum resources for scheduling.
- `limits`: Caps to prevent overuse.

#### Example 2: ResourceQuota
```yaml
# File: workloads/namespace-quota.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: app-quota
  namespace: default
spec:
  hard:
    requests.cpu: "4"
    requests.memory: "4Gi"
    limits.cpu: "8"
    limits.memory: "8Gi"
    pods: "10"
```

**Critical Fields**:
- `hard`: Enforces namespace limits.

#### Example 3: LimitRange
```yaml
# File: workloads/namespace-limitrange.yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: app-limits
  namespace: default
spec:
  limits:
  - type: Pod
    max:
      cpu: "2"
      memory: "2Gi"
    min:
      cpu: "100m"
      memory: "128Mi"
  - type: Container
    default:
      cpu: "500m"
      memory: "512Mi"
    defaultRequest:
      cpu: "200m"
      memory: "256Mi"
```

**Critical Fields**:
- `default`: Applies if unspecified.
- `min`, `max`: Enforce boundaries.

#### Example 4: Debug Pod
```yaml
# File: workloads/debug-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: debug-pod
  namespace: default
spec:
  containers:
  - name: debug
    image: busybox
    command: ["sh", "-c", "sleep 3600"]
```

**Purpose**: Debug resource issues.

---

### 4. Critical Commands
Key commands for requests, limits, and quotas:

| Command | Description | Exam Tip |
|---------|-------------|----------|
| `kubectl apply -f <file>` | Apply pod/quota. | Set up resources. |
| `kubectl get pods` | Check pod status. | Verify scheduling. |
| `kubectl describe pod <pod>` | Resource configs. | Debug limits. |
| `kubectl get resourcequota` | List quotas. | Check limits. |
| `kubectl describe resourcequota <name>` | Quota usage. | Debug violations. |
| `kubectl get limitrange` | List limit ranges. | Verify defaults. |
| `kubectl describe limitrange <name>` | Limit details. | Check constraints. |
| `kubectl top pods` | Resource usage. | Validate limits. |
| `kubectl describe node` | Node capacity. | Check allocatable. |
| `crictl ps` | Pod states. | Debug Pending. |
| `crictl logs <id>` | OOM errors. | Debug kills. |
| `crictl inspect <id>` | Resource configs. | Verify settings. |

---

### 5. Step-by-Step Procedures
Here’s how to configure, test, and troubleshoot resource management.

#### Scenario 1: Configure Pod with Requests/Limits
**Step 1: Deploy Pod**
```bash
kubectl apply -f workloads/resource-pod.yaml
kubectl get pods
# Output: resource-app   1/1   Running
```

**Step 2: Verify**
```bash
kubectl describe pod resource-app
# Output: Requests: cpu: 500m, memory: 512Mi
#         Limits: cpu: 1, memory: 1Gi
kubectl top pod resource-app
# Output: CPU: <1, Memory: <1Gi
```

#### Scenario 2: Apply ResourceQuota
**Step 1: Create Quota**
```bash
kubectl apply -f workloads/namespace-quota.yaml
kubectl describe resourcequota app-quota
# Output: pods: 0/10, requests.memory: 0/4Gi
```

**Step 2: Test Quota**
```bash
cat <<EOF > excess-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: excess-app
spec:
  containers:
  - name: app
    image: nginx:1.25
    resources:
      requests:
        cpu: "5"
        memory: "5Gi"
EOF
kubectl apply -f excess-pod.yaml
kubectl get pods
# Output: excess-app   0/1   Pending
kubectl describe pod excess-app
# Output: Events: "exceeded quota: app-quota"
```

**Step 3: Fix**
```bash
kubectl edit pod/excess-app
# Correct: requests.cpu: "1", memory: "1Gi"
kubectl get pods
# Output: excess-app   1/1   Running
```

#### Scenario 3: Debug OOM Kill
**Step 1: Deploy Low-Limit Pod**
```bash
cat <<EOF > oom-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: oom-app
spec:
  containers:
  - name: app
    image: polinux/stress
    args: ["--vm", "1", "--vm-bytes", "512M"]
    resources:
      limits:
        memory: "256Mi"
EOF
kubectl apply -f oom-pod.yaml
```

**Step 2: Check**
```bash
kubectl get pods
# Output: oom-app   0/1   CrashLoopBackOff
kubectl describe pod oom-app
# Output: Events: "OOMKilled"
```

**Step 3: Runtime Debugging**
```bash
kubectl apply -f workloads/debug-pod.yaml
kubectl exec -it debug-pod -- sh
crictl ps -a
# Output: oom-app   Exited   OOMKilled
crictl logs <container-id>
# Output: Memory allocation failed
```

**Step 4: Fix**
```bash
kubectl edit pod/oom-app
# Correct: limits.memory: "512Mi"
kubectl get pods
# Output: oom-app   1/1   Running
```

#### Scenario 4: Apply LimitRange
**Step 1: Create LimitRange**
```bash
kubectl apply -f workloads/namespace-limitrange.yaml
kubectl describe limitrange app-limits
# Output: default: cpu: 500m, memory: 512Mi
```

**Step 2: Deploy Pod without Limits**
```bash
cat <<EOF > no-limit-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: no-limit-app
spec:
  containers:
  - name: app
    image: nginx:1.25
EOF
kubectl apply -f no-limit-pod.yaml
```

**Step 3: Verify**
```bash
kubectl describe pod no-limit-app
# Output: Requests: cpu: 200m, memory: 256Mi
#         Limits: cpu: 500m, memory: 512Mi
```

---

### 6. Exam-Style Questions
Below are CKA-style questions, with runtime debugging included.

#### Question 1: Task-Based (6 minutes)
**Task**: Configure a pod with `500m` CPU request, `1Gi` memory limit.

**Steps**:
```bash
kubectl apply -f workloads/resource-pod.yaml
kubectl describe pod resource-app
# Output: Requests: cpu: 500m, Limits: memory: 1Gi
kubectl get pods
# Output: resource-app   1/1   Running
```

#### Question 2: Task-Based (7 minutes)
**Task**: Create a ResourceQuota for 10 pods, 4Gi memory.

**Steps**:
```bash
kubectl apply -f workloads/namespace-quota.yaml
kubectl describe resourcequota app-quota
# Output: pods: 0/10, requests.memory: 0/4Gi
```

#### Question 3: Troubleshooting (8 minutes)
**Task**: Fix a pod stuck in `Pending` due to quota.

**Steps**:
```bash
kubectl apply -f workloads/namespace-quota.yaml
kubectl apply -f excess-pod.yaml
kubectl get pods
# Output: excess-app   0/1   Pending

kubectl describe pod excess-app
# Output: "exceeded quota"

# Fix
kubectl edit pod/excess-app
# Correct: requests.cpu: "1", memory: "1Gi"

kubectl get pods
# Output: excess-app   1/1   Running
```

#### Question 4: Validation (5 minutes)
**Task**: Verify LimitRange applies defaults.

**Steps**:
```bash
kubectl apply -f workloads/namespace-limitrange.yaml
kubectl apply -f no-limit-pod.yaml
kubectl describe pod no-limit-app
# Output: Limits: cpu: 500m, memory: 512Mi
kubectl get pods
# Output: no-limit-app   1/1   Running
```

---

### 7. Important Key Points to Remember
- **Requests/Limits**:
  - `requests`: Minimum for scheduling.
  - `limits`: Caps (OOM for memory, throttle for CPU).
- **ResourceQuota**:
  - Limits namespace (e.g., `pods`, `requests.memory`).
  - Blocks creation if exceeded.
- **LimitRange**:
  - Sets default/min/max per pod.
  - Auto-applies if unspecified.
- **Runtime Debugging**:
  - `crictl ps/logs`: OOM, Pending issues.
  - Use for resource failures.
- **Exam Focus**:
  - Configure requests, quotas.
  - Fix scheduling, OOM issues.
  - Validate constraints.

---

### 8. Common Mistakes to Avoid
- **Mistake**: No `requests` causing overscheduling.
  - **Fix**: Add `resources.requests`.
- **Mistake**: Low `limits.memory` triggering OOM.
  - **Fix**: Increase `limits.memory`.
- **Mistake**: Ignoring quota limits.
  - **Fix**: Check `kubectl describe resourcequota`.
- **Mistake**: Wrong units (e.g., `m` vs. `Mi`).
  - **Fix**: Use `m` for CPU, `Mi` for memory.
- **Mistake**: Missing runtime logs.
  - **Fix**: Check `crictl logs`.

**Exam Traps**:
- Exceeding quota unnoticed.
- Incorrect `defaultRequest` in LimitRange.
- Forgetting to check node capacity.

---

### 9. Troubleshooting Tips
- **Pod Pending**:
  - Check: `kubectl describe pod`.
  - Causes: Quota violation, insufficient requests.
  - Fix: Adjust requests, check quota.
- **OOM Kills**:
  - Check: `kubectl describe pod`, `crictl logs`.
  - Causes: Low `limits.memory`.
  - Fix: Increase limits.
- **CPU Throttling**:
  - Check: `kubectl top pod`.
  - Causes: Low `limits.cpu`.
  - Fix: Increase or remove `limits.cpu`.
- **Runtime Issues**:
  - Check: `crictl ps/inspect`.
  - Causes: Resource misconfigs.
  - Fix: Correct YAML, restart runtime.
- **Tools**:
  - `kubectl describe`: Quota, limit issues.
  - `kubectl top`: Usage validation.
  - `crictl`: Crash diagnostics.

**Debugging Checklist**:
1. Check pod (`kubectl get pods`).
2. Inspect events (`kubectl describe pod`).
3. Verify quota (`kubectl describe resourcequota`).
4. Debug runtime (`crictl logs`).
5. Fix YAML (`kubectl edit`).
6. Validate (`kubectl describe pod`).

---

### 10. Additional Resources
- **Kubernetes Docs**:
  - [Manage Resources](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)
  - [Resource Quotas](https://kubernetes.io/docs/concepts/policy/resource-quotas/)
  - [Limit Ranges](https://kubernetes.io/docs/concepts/policy/limit-range/)
- **Practice Tools**:
  - **Minikube**: Test quotas.
  - **KillerCoda/KodeKloud**: CKA labs.
  - **Kind**: Multi-node setups.
- **Community**:
  - CNCF Slack: #kubernetes-users, #cka-prep.
  - Kubernetes GitHub: Resource issues.
  - X posts: Search #KubernetesQuotas, #CKA.

---

### 11. GitHub Repo Integration
Here’s how to organize this topic in your GitHub repo.

#### Folder Structure
```
cka-prep/
├── storage/
├── troubleshoot/
├── workloads/
│   ├── my-app-deployment.yaml
│   ├── rolling-deployment.yaml
│   ├── debug-pod.yaml
│   ├── private-deployment.yaml
│   ├── registry-secret.yaml
│   ├── Dockerfile
│   ├── app.py
│   ├── logging-daemonset.yaml
│   ├── backup-cronjob.yaml
│   ├── batch-job.yaml
│   ├── sidecar-pod.yaml
│   ├── init-pod.yaml
│   ├── blue-deployment.yaml
│   ├── green-deployment.yaml
│   ├── blue-green-service.yaml
│   ├── stable-deployment.yaml
│   ├── canary-deployment.yaml
│   ├── canary-service.yaml
│   ├── rolling-update-deployment.yaml
│   ├── recreate-deployment.yaml
│   ├── app-configmap.yaml
│   ├── app-secret.yaml
│   ├── configured-pod.yaml
│   ├── private-pod.yaml
│   ├── immutable-configmap.yaml
│   ├── config-pod.yaml
│   ├── tls-secret.yaml
│   ├── secret-pod.yaml
│   ├── scalable-deployment.yaml
│   ├── web-hpa.yaml
│   ├── load-pod.yaml
│   ├── robust-deployment.yaml
│   ├── slow-pod.yaml
│   ├── limited-pod.yaml
│   ├── affinity-deployment.yaml
│   ├── toleration-pod.yaml
│   ├── selector-pod.yaml
│   ├── resource-pod.yaml
│   ├── namespace-quota.yaml
│   ├── namespace-limitrange.yaml
│   ├── README.md
├── README.md
```

#### README Content (workloads/README.md, Updated)
```markdown
# Workloads & Scheduling: Requests, Limits, Quotas

## Theory
- **Requests/Limits**: Scheduling, caps.
- **ResourceQuota**: Namespace limits.
- **LimitRange**: Pod defaults, bounds.

## Files
- `resource-pod.yaml`: Requests/limits.
- `namespace-quota.yaml`: ResourceQuota.
- `namespace-limitrange.yaml`: LimitRange.
- Others: `affinity-deployment.yaml`, etc.

## Procedures
1. Deploy: `kubectl apply -f`
2. Test: Create excess pods.
3. Debug: `kubectl describe`, `crictl`
4. Fix: `kubectl edit`
5. Validate: `kubectl top pod`

## Key Points
- `requests`: Minimum for scheduling.
- `limits`: OOM, throttling.
- Quotas block if exceeded.

## Common Mistakes
- No `requests`.
- Low `limits.memory`.
- Ignoring quotas.

## Troubleshooting
- Pending? Check quota.
- OOM? Increase `limits.memory`.

## Questions
1. Set requests/limits.
2. Create quota.
3. Fix quota violation.
4. Verify LimitRange.
```

#### File Comments (resource-pod.yaml)
```yaml
# resource-pod.yaml
# Pod with CPU/memory requests and limits
# Verify: kubectl describe pod resource-app
# Use: Practice resource management
```

---

### Comprehensive Summary
This topic equips you to manage **requests, limits, and quotas**, ensuring efficient resource allocation and namespace constraints. You’ve learned:
- How to configure **requests** and **limits** for pod scheduling and stability.
- How to enforce **ResourceQuotas** and **LimitRanges** for namespace control.
- How to troubleshoot scheduling failures, OOM kills, and quota issues using `kubectl` and `crictl`.
- Practical skills for validating resource enforcement in real scenarios.

**Practice Plan**:
- Deploy `resource-pod.yaml`, `namespace-quota.yaml`, and `namespace-limitrange.yaml` (Minikube, Kind).
- Simulate failures: exceed quotas, trigger OOM. Debug with `kubectl describe`, `crictl`.
- Time yourself on the exam questions (<20 minutes total).
- Use KillerCoda/KodeKloud for quota labs.

**Next Steps**:
- Move to the next topic in Workloads & Scheduling (if any remain) or the next CKA section (e.g., **Cluster Architecture, Installation & Configuration**).
- Practice mixed scenarios (e.g., quotas + affinity).
- Let me know if you want more runtime debugging, quota edge cases, or a section review.

---

This response covers **Understand Requests, Limits, Quotas** comprehensively, with runtime debugging included, tailored for your CKA prep and GitHub repo. We’re crushing Workloads & Scheduling—awesome work! Please share the next topic or section, and I’ll keep delivering. Any tweaks, more debugging, or specific resource scenarios? Let’s keep this prep unstoppable! 😊
